{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Titanic - Machine Learning Challenge Core\n",
        "\n",
        "## Descripci√≥n del Proyecto\n",
        "\n",
        "Este notebook implementa un an√°lisis completo del dataset de Titanic para predecir la supervivencia de los pasajeros. El proyecto incluye:\n",
        "\n",
        "1. **Exploraci√≥n y An√°lisis de Datos (EDA)**\n",
        "2. **Preprocesamiento de Datos**\n",
        "3. **Implementaci√≥n y Evaluaci√≥n de 5 Modelos de ML**\n",
        "4. **Benchmark y Comparaci√≥n de Rendimiento**\n",
        "5. **Generaci√≥n de Predicciones**\n",
        "\n",
        "## Modelos Implementados\n",
        "- Regresi√≥n Log√≠stica\n",
        "- K-Nearest Neighbors (KNN)\n",
        "- √Årbol de Decisi√≥n\n",
        "- XGBoost\n",
        "- LightGBM\n",
        "\n",
        "---\n",
        "\n",
        "**Autor:** Challenge T√©cnico ML  \n",
        "**Dataset:** Titanic - Machine Learning from Disaster (Kaggle)  \n",
        "**Objetivo:** Predecir supervivencia con accuracy > 80%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Librer√≠as de Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Modelos de Machine Learning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")\n",
        "print(f\"Versi√≥n de pandas: {pd.__version__}\")\n",
        "print(f\"Versi√≥n de numpy: {np.__version__}\")\n",
        "print(f\"Versi√≥n de XGBoost: {xgb.__version__}\")\n",
        "print(f\"Versi√≥n de LightGBM: {lgb.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Carga y Exploraci√≥n Inicial de Datos\n",
        "\n",
        "En esta secci√≥n cargaremos los datasets y realizaremos una exploraci√≥n inicial para entender la estructura y caracter√≠sticas de los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga de datos\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "submission_sample = pd.read_csv('gender_submission.csv')\n",
        "\n",
        "print(\"üìä Datasets cargados correctamente\")\n",
        "print(f\"üöÇ Datos de entrenamiento: {train_df.shape}\")\n",
        "print(f\"üß™ Datos de prueba: {test_df.shape}\")\n",
        "print(f\"üìù Muestra de submission: {submission_sample.shape}\")\n",
        "\n",
        "# Primera visualizaci√≥n de los datos\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PRIMERAS 5 FILAS DEL DATASET DE ENTRENAMIENTO\")\n",
        "print(\"=\"*50)\n",
        "display(train_df.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INFORMACI√ìN GENERAL DEL DATASET\")\n",
        "print(\"=\"*50)\n",
        "print(train_df.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estad√≠sticas descriptivas\n",
        "print(\"=\"*60)\n",
        "print(\"ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES NUM√âRICAS\")\n",
        "print(\"=\"*60)\n",
        "display(train_df.describe())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES CATEG√ìRICAS\")\n",
        "print(\"=\"*60)\n",
        "display(train_df.describe(include=['O']))\n",
        "\n",
        "# An√°lisis de valores faltantes\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AN√ÅLISIS DE VALORES FALTANTES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "missing_train = train_df.isnull().sum()\n",
        "missing_test = test_df.isnull().sum()\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Train_Missing': missing_train,\n",
        "    'Train_Percentage': (missing_train / len(train_df)) * 100,\n",
        "    'Test_Missing': missing_test,\n",
        "    'Test_Percentage': (missing_test / len(test_df)) * 100\n",
        "})\n",
        "\n",
        "missing_df = missing_df[missing_df['Train_Missing'] > 0].sort_values('Train_Missing', ascending=False)\n",
        "display(missing_df)\n",
        "\n",
        "# Distribuci√≥n de la variable objetivo\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DISTRIBUCI√ìN DE LA VARIABLE OBJETIVO (SURVIVED)\")\n",
        "print(\"=\"*60)\n",
        "survival_counts = train_df['Survived'].value_counts()\n",
        "survival_percentage = train_df['Survived'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(f\"No sobrevivieron (0): {survival_counts[0]} pasajeros ({survival_percentage[0]:.1f}%)\")\n",
        "print(f\"Sobrevivieron (1): {survival_counts[1]} pasajeros ({survival_percentage[1]:.1f}%)\")\n",
        "print(f\"Tasa de supervivencia: {survival_percentage[1]:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. An√°lisis Exploratorio de Datos (EDA)\n",
        "\n",
        "En esta secci√≥n realizaremos visualizaciones para entender mejor las relaciones entre variables y identificar patrones que nos ayuden en la construcci√≥n de nuestros modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaci√≥n de la distribuci√≥n de supervivencia\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Distribuci√≥n general de supervivencia\n",
        "axes[0,0].pie(survival_counts.values, labels=['No Sobrevivi√≥', 'Sobrevivi√≥'], \n",
        "              autopct='%1.1f%%', startangle=90, colors=['#ff6b6b', '#4ecdc4'])\n",
        "axes[0,0].set_title('Distribuci√≥n de Supervivencia', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 2. Supervivencia por g√©nero\n",
        "survival_sex = pd.crosstab(train_df['Sex'], train_df['Survived'])\n",
        "survival_sex.plot(kind='bar', ax=axes[0,1], color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[0,1].set_title('Supervivencia por G√©nero', fontsize=14, fontweight='bold')\n",
        "axes[0,1].set_xlabel('G√©nero')\n",
        "axes[0,1].set_ylabel('N√∫mero de Pasajeros')\n",
        "axes[0,1].legend(['No Sobrevivi√≥', 'Sobrevivi√≥'])\n",
        "axes[0,1].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 3. Supervivencia por clase\n",
        "survival_class = pd.crosstab(train_df['Pclass'], train_df['Survived'])\n",
        "survival_class.plot(kind='bar', ax=axes[1,0], color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[1,0].set_title('Supervivencia por Clase', fontsize=14, fontweight='bold')\n",
        "axes[1,0].set_xlabel('Clase')\n",
        "axes[1,0].set_ylabel('N√∫mero de Pasajeros')\n",
        "axes[1,0].legend(['No Sobrevivi√≥', 'Sobrevivi√≥'])\n",
        "axes[1,0].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 4. Distribuci√≥n de edades\n",
        "axes[1,1].hist([train_df[train_df['Survived']==0]['Age'].dropna(), \n",
        "                train_df[train_df['Survived']==1]['Age'].dropna()], \n",
        "               bins=30, alpha=0.7, label=['No Sobrevivi√≥', 'Sobrevivi√≥'],\n",
        "               color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[1,1].set_title('Distribuci√≥n de Edades por Supervivencia', fontsize=14, fontweight='bold')\n",
        "axes[1,1].set_xlabel('Edad')\n",
        "axes[1,1].set_ylabel('Frecuencia')\n",
        "axes[1,1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis m√°s detallado de correlaciones y patrones\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Mapa de calor de correlaciones\n",
        "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = train_df[numeric_cols].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "            square=True, ax=axes[0,0])\n",
        "axes[0,0].set_title('Matriz de Correlaci√≥n', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 2. Supervivencia por puerto de embarque\n",
        "survival_embarked = pd.crosstab(train_df['Embarked'], train_df['Survived'])\n",
        "survival_embarked.plot(kind='bar', ax=axes[0,1], color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[0,1].set_title('Supervivencia por Puerto de Embarque', fontsize=14, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Puerto de Embarque')\n",
        "axes[0,1].set_ylabel('N√∫mero de Pasajeros')\n",
        "axes[0,1].legend(['No Sobrevivi√≥', 'Sobrevivi√≥'])\n",
        "axes[0,1].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 3. Distribuci√≥n de tarifas por supervivencia\n",
        "train_df.boxplot(column='Fare', by='Survived', ax=axes[0,2])\n",
        "axes[0,2].set_title('Distribuci√≥n de Tarifas por Supervivencia', fontsize=14, fontweight='bold')\n",
        "axes[0,2].set_xlabel('Supervivencia')\n",
        "axes[0,2].set_ylabel('Tarifa')\n",
        "\n",
        "# 4. Supervivencia por n√∫mero de hermanos/c√≥nyuges\n",
        "survival_sibsp = pd.crosstab(train_df['SibSp'], train_df['Survived'])\n",
        "survival_sibsp.plot(kind='bar', ax=axes[1,0], color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[1,0].set_title('Supervivencia por SibSp', fontsize=14, fontweight='bold')\n",
        "axes[1,0].set_xlabel('N√∫mero de Hermanos/C√≥nyuges')\n",
        "axes[1,0].set_ylabel('N√∫mero de Pasajeros')\n",
        "axes[1,0].legend(['No Sobrevivi√≥', 'Sobrevivi√≥'])\n",
        "axes[1,0].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 5. Supervivencia por n√∫mero de padres/hijos\n",
        "survival_parch = pd.crosstab(train_df['Parch'], train_df['Survived'])\n",
        "survival_parch.plot(kind='bar', ax=axes[1,1], color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[1,1].set_title('Supervivencia por Parch', fontsize=14, fontweight='bold')\n",
        "axes[1,1].set_xlabel('N√∫mero de Padres/Hijos')\n",
        "axes[1,1].set_ylabel('N√∫mero de Pasajeros')\n",
        "axes[1,1].legend(['No Sobrevivi√≥', 'Sobrevivi√≥'])\n",
        "axes[1,1].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 6. An√°lisis combinado: G√©nero y Clase\n",
        "survival_sex_class = train_df.groupby(['Sex', 'Pclass'])['Survived'].mean().unstack()\n",
        "survival_sex_class.plot(kind='bar', ax=axes[1,2], color=['#ff9999', '#66b3ff', '#99ff99'])\n",
        "axes[1,2].set_title('Tasa de Supervivencia por G√©nero y Clase', fontsize=14, fontweight='bold')\n",
        "axes[1,2].set_xlabel('G√©nero')\n",
        "axes[1,2].set_ylabel('Tasa de Supervivencia')\n",
        "axes[1,2].legend(['Clase 1', 'Clase 2', 'Clase 3'])\n",
        "axes[1,2].tick_params(axis='x', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Insights del EDA\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîç INSIGHTS PRINCIPALES DEL AN√ÅLISIS EXPLORATORIO\")\n",
        "print(\"=\"*80)\n",
        "print(\"1. Las mujeres tuvieron una tasa de supervivencia significativamente mayor que los hombres\")\n",
        "print(\"2. Los pasajeros de primera clase tuvieron mayor probabilidad de supervivencia\")\n",
        "print(\"3. Los ni√±os (edades menores) tuvieron mejores tasas de supervivencia\")\n",
        "print(\"4. Los pasajeros que pagaron tarifas m√°s altas tendieron a sobrevivir m√°s\")\n",
        "print(\"5. El puerto de embarque muestra algunas diferencias en supervivencia\")\n",
        "print(\"6. Tener muchos hermanos/c√≥nyuges o padres/hijos puede haber afectado la supervivencia\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Preprocesamiento de Datos\n",
        "\n",
        "En esta secci√≥n realizaremos la limpieza y transformaci√≥n de los datos para prepararlos para el entrenamiento de modelos de machine learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para preprocesar los datos\n",
        "def preprocess_data(df, is_train=True):\n",
        "    \"\"\"\n",
        "    Funci√≥n para preprocesar los datos de Titanic\n",
        "    \"\"\"\n",
        "    # Crear una copia para no modificar el original\n",
        "    data = df.copy()\n",
        "    \n",
        "    # 1. Feature Engineering - Extraer t√≠tulo del nombre\n",
        "    data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "    \n",
        "    # Agrupar t√≠tulos raros\n",
        "    title_mapping = {\n",
        "        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
        "        'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',\n",
        "        'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare',\n",
        "        'Jonkheer': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs',\n",
        "        'Capt': 'Rare', 'Sir': 'Rare'\n",
        "    }\n",
        "    data['Title'] = data['Title'].map(title_mapping).fillna('Rare')\n",
        "    \n",
        "    # 2. Crear variable de tama√±o de familia\n",
        "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
        "    \n",
        "    # 3. Crear variable de si viaja solo\n",
        "    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n",
        "    \n",
        "    # 4. Agrupar tama√±os de familia\n",
        "    data['FamilySizeGroup'] = 'Medium'\n",
        "    data.loc[data['FamilySize'] == 1, 'FamilySizeGroup'] = 'Alone'\n",
        "    data.loc[data['FamilySize'] >= 5, 'FamilySizeGroup'] = 'Large'\n",
        "    \n",
        "    # 5. Extraer informaci√≥n de la cabina (si tiene cabina o no)\n",
        "    data['HasCabin'] = (~data['Cabin'].isna()).astype(int)\n",
        "    \n",
        "    # 6. Imputar valores faltantes\n",
        "    # Edad: usar la mediana por t√≠tulo\n",
        "    if data['Age'].isna().any():\n",
        "        age_title_mapping = data.groupby('Title')['Age'].median().to_dict()\n",
        "        for title in age_title_mapping:\n",
        "            data.loc[(data['Age'].isna()) & (data['Title'] == title), 'Age'] = age_title_mapping[title]\n",
        "    \n",
        "    # Embarked: usar la moda\n",
        "    if data['Embarked'].isna().any():\n",
        "        data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "    \n",
        "    # Fare: usar la mediana\n",
        "    if data['Fare'].isna().any():\n",
        "        data['Fare'].fillna(data['Fare'].median(), inplace=True)\n",
        "    \n",
        "    # 7. Crear grupos de edad\n",
        "    data['AgeGroup'] = 'Adult'\n",
        "    data.loc[data['Age'] <= 16, 'AgeGroup'] = 'Child'\n",
        "    data.loc[data['Age'] >= 60, 'AgeGroup'] = 'Elderly'\n",
        "    \n",
        "    # 8. Crear grupos de tarifa\n",
        "    data['FareGroup'] = 'Low'\n",
        "    data.loc[data['Fare'] > data['Fare'].quantile(0.33), 'FareGroup'] = 'Medium'\n",
        "    data.loc[data['Fare'] > data['Fare'].quantile(0.66), 'FareGroup'] = 'High'\n",
        "    \n",
        "    # 9. Seleccionar caracter√≠sticas finales\n",
        "    features_to_use = [\n",
        "        'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked',\n",
        "        'Title', 'FamilySize', 'IsAlone', 'FamilySizeGroup', 'HasCabin',\n",
        "        'AgeGroup', 'FareGroup'\n",
        "    ]\n",
        "    \n",
        "    # Si es conjunto de entrenamiento, incluir la variable objetivo\n",
        "    if is_train:\n",
        "        features_to_use.append('Survived')\n",
        "    \n",
        "    return data[features_to_use]\n",
        "\n",
        "# Aplicar preprocesamiento\n",
        "print(\"üîß Aplicando preprocesamiento a los datos...\")\n",
        "train_processed = preprocess_data(train_df, is_train=True)\n",
        "test_processed = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "print(f\"‚úÖ Preprocesamiento completado\")\n",
        "print(f\"üìä Datos de entrenamiento procesados: {train_processed.shape}\")\n",
        "print(f\"üß™ Datos de prueba procesados: {test_processed.shape}\")\n",
        "\n",
        "# Mostrar las primeras filas de los datos procesados\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PRIMERAS 5 FILAS DE LOS DATOS PROCESADOS\")\n",
        "print(\"=\"*60)\n",
        "display(train_processed.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codificaci√≥n de variables categ√≥ricas y preparaci√≥n final\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import sklearn\n",
        "\n",
        "# Corregir la importaci√≥n de sklearn\n",
        "print(f\"Versi√≥n de scikit-learn: {sklearn.__version__}\")\n",
        "\n",
        "# Combinar datos de entrenamiento y prueba para codificaci√≥n consistente\n",
        "all_data = pd.concat([train_processed.drop('Survived', axis=1), test_processed], ignore_index=True)\n",
        "\n",
        "# Identificar variables categ√≥ricas\n",
        "categorical_features = ['Sex', 'Embarked', 'Title', 'FamilySizeGroup', 'AgeGroup', 'FareGroup']\n",
        "print(f\"Variables categ√≥ricas a codificar: {categorical_features}\")\n",
        "\n",
        "# Aplicar Label Encoding a variables categ√≥ricas\n",
        "label_encoders = {}\n",
        "for feature in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    all_data[feature] = le.fit_transform(all_data[feature])\n",
        "    label_encoders[feature] = le\n",
        "    print(f\"‚úÖ Codificada variable: {feature}\")\n",
        "\n",
        "# Separar de nuevo los conjuntos\n",
        "train_encoded = all_data.iloc[:len(train_processed)].copy()\n",
        "test_encoded = all_data.iloc[len(train_processed):].copy()\n",
        "\n",
        "# Agregar de nuevo la variable objetivo al conjunto de entrenamiento\n",
        "train_encoded['Survived'] = train_processed['Survived']\n",
        "\n",
        "# Preparar X e y para el entrenamiento\n",
        "X = train_encoded.drop('Survived', axis=1)\n",
        "y = train_encoded['Survived']\n",
        "\n",
        "# Datos de prueba para predicci√≥n final\n",
        "X_test_final = test_encoded.copy()\n",
        "\n",
        "print(f\"\\nüìä Datos finales preparados:\")\n",
        "print(f\"üöÇ X_train shape: {X.shape}\")\n",
        "print(f\"üéØ y_train shape: {y.shape}\")\n",
        "print(f\"üß™ X_test shape: {X_test_final.shape}\")\n",
        "\n",
        "# Verificar que no hay valores faltantes\n",
        "print(f\"\\nüîç Valores faltantes en X_train: {X.isnull().sum().sum()}\")\n",
        "print(f\"üîç Valores faltantes en X_test: {X_test_final.isnull().sum().sum()}\")\n",
        "\n",
        "# Mostrar informaci√≥n de las caracter√≠sticas finales\n",
        "print(f\"\\nüìã Caracter√≠sticas finales ({len(X.columns)}):\")\n",
        "for i, col in enumerate(X.columns, 1):\n",
        "    print(f\"{i:2d}. {col}\")\n",
        "    \n",
        "# Divisi√≥n del conjunto de entrenamiento\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nüîÑ Divisi√≥n de datos completada:\")\n",
        "print(f\"üìä X_train: {X_train.shape}\")\n",
        "print(f\"üìä X_val: {X_val.shape}\")\n",
        "print(f\"üéØ y_train: {y_train.shape}\")\n",
        "print(f\"üéØ y_val: {y_val.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Implementaci√≥n y Evaluaci√≥n de Modelos\n",
        "\n",
        "En esta secci√≥n implementaremos y evaluaremos 5 modelos diferentes de machine learning:\n",
        "1. **Regresi√≥n Log√≠stica**\n",
        "2. **K-Nearest Neighbors (KNN)**\n",
        "3. **√Årbol de Decisi√≥n**\n",
        "4. **XGBoost**\n",
        "5. **LightGBM**\n",
        "\n",
        "Cada modelo ser√° evaluado usando validaci√≥n cruzada y m√©tricas de rendimiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para evaluar modelos\n",
        "def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):\n",
        "    \"\"\"\n",
        "    Funci√≥n para entrenar y evaluar un modelo\n",
        "    \"\"\"\n",
        "    # Entrenar el modelo\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predicciones en entrenamiento y validaci√≥n\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    \n",
        "    # Calcular accuracies\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    \n",
        "    # Validaci√≥n cruzada\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    \n",
        "    # Crear diccionario de resultados\n",
        "    results = {\n",
        "        'Model': model_name,\n",
        "        'Train_Accuracy': train_accuracy,\n",
        "        'Validation_Accuracy': val_accuracy,\n",
        "        'CV_Mean': cv_scores.mean(),\n",
        "        'CV_Std': cv_scores.std(),\n",
        "        'Overfitting': train_accuracy - val_accuracy\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nüìä {model_name}\")\n",
        "    print(f\"üöÇ Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"üß™ Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"üîÑ CV Mean: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
        "    print(f\"‚ö†Ô∏è  Overfitting: {train_accuracy - val_accuracy:.4f}\")\n",
        "    \n",
        "    return results, model\n",
        "\n",
        "# Inicializar lista para almacenar resultados\n",
        "results_list = []\n",
        "trained_models = {}\n",
        "\n",
        "print(\"üöÄ COMENZANDO ENTRENAMIENTO DE MODELOS\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. REGRESI√ìN LOG√çSTICA\n",
        "print(\"\\n1Ô∏è‚É£ REGRESI√ìN LOG√çSTICA\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Escalar caracter√≠sticas para Regresi√≥n Log√≠stica\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Crear y evaluar modelo\n",
        "logistic_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "results, trained_logistic = evaluate_model(logistic_model, X_train_scaled, X_val_scaled, y_train, y_val, \"Logistic Regression\")\n",
        "results_list.append(results)\n",
        "trained_models['Logistic Regression'] = (trained_logistic, scaler)\n",
        "\n",
        "# 2. K-NEAREST NEIGHBORS\n",
        "print(\"\\n2Ô∏è‚É£ K-NEAREST NEIGHBORS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# KNN tambi√©n necesita datos escalados\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "results, trained_knn = evaluate_model(knn_model, X_train_scaled, X_val_scaled, y_train, y_val, \"K-Nearest Neighbors\")\n",
        "results_list.append(results)\n",
        "trained_models['K-Nearest Neighbors'] = (trained_knn, scaler)\n",
        "\n",
        "# 3. √ÅRBOL DE DECISI√ìN\n",
        "print(\"\\n3Ô∏è‚É£ √ÅRBOL DE DECISI√ìN\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# √Årbol de Decisi√≥n no necesita escalado\n",
        "tree_model = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=5)\n",
        "results, trained_tree = evaluate_model(tree_model, X_train, X_val, y_train, y_val, \"Decision Tree\")\n",
        "results_list.append(results)\n",
        "trained_models['Decision Tree'] = (trained_tree, None)\n",
        "\n",
        "# 4. RANDOM FOREST (Como alternativa robusta al √°rbol simple)\n",
        "print(\"\\n4Ô∏è‚É£ RANDOM FOREST\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Random Forest tampoco necesita escalado\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "results, trained_rf = evaluate_model(rf_model, X_train, X_val, y_train, y_val, \"Random Forest\")\n",
        "results_list.append(results)\n",
        "trained_models['Random Forest'] = (trained_rf, None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. XGBOOST\n",
        "print(\"\\n5Ô∏è‚É£ XGBOOST\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "results, trained_xgb = evaluate_model(xgb_model, X_train, X_val, y_train, y_val, \"XGBoost\")\n",
        "results_list.append(results)\n",
        "trained_models['XGBoost'] = (trained_xgb, None)\n",
        "\n",
        "# 6. LIGHTGBM\n",
        "print(\"\\n6Ô∏è‚É£ LIGHTGBM\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# LightGBM\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    verbosity=-1\n",
        ")\n",
        "results, trained_lgb = evaluate_model(lgb_model, X_train, X_val, y_train, y_val, \"LightGBM\")\n",
        "results_list.append(results)\n",
        "trained_models['LightGBM'] = (trained_lgb, None)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ ENTRENAMIENTO DE TODOS LOS MODELOS COMPLETADO\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Benchmark y Comparaci√≥n de Modelos\n",
        "\n",
        "En esta secci√≥n compararemos el rendimiento de todos los modelos implementados y realizaremos an√°lisis adicionales para seleccionar el mejor modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear DataFrame con resultados del benchmark\n",
        "results_df = pd.DataFrame(results_list)\n",
        "results_df = results_df.round(4)\n",
        "\n",
        "print(\"üìä TABLA DE COMPARACI√ìN DE MODELOS\")\n",
        "print(\"=\"*80)\n",
        "display(results_df)\n",
        "\n",
        "# Encontrar el mejor modelo\n",
        "best_model_idx = results_df['CV_Mean'].idxmax()\n",
        "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
        "best_cv_score = results_df.loc[best_model_idx, 'CV_Mean']\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
        "print(f\"üéØ CV Score: {best_cv_score:.4f}\")\n",
        "\n",
        "# Visualizaci√≥n de resultados\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Comparaci√≥n de Accuracy de Validaci√≥n Cruzada\n",
        "axes[0,0].barh(results_df['Model'], results_df['CV_Mean'], color='skyblue', alpha=0.7)\n",
        "axes[0,0].set_xlabel('CV Mean Accuracy')\n",
        "axes[0,0].set_title('Validaci√≥n Cruzada - Accuracy Media', fontweight='bold')\n",
        "for i, v in enumerate(results_df['CV_Mean']):\n",
        "    axes[0,0].text(v + 0.001, i, f'{v:.3f}', va='center')\n",
        "\n",
        "# 2. Comparaci√≥n Train vs Validation Accuracy\n",
        "x = np.arange(len(results_df))\n",
        "width = 0.35\n",
        "axes[0,1].bar(x - width/2, results_df['Train_Accuracy'], width, label='Train', alpha=0.7)\n",
        "axes[0,1].bar(x + width/2, results_df['Validation_Accuracy'], width, label='Validation', alpha=0.7)\n",
        "axes[0,1].set_xlabel('Modelos')\n",
        "axes[0,1].set_ylabel('Accuracy')\n",
        "axes[0,1].set_title('Train vs Validation Accuracy', fontweight='bold')\n",
        "axes[0,1].set_xticks(x)\n",
        "axes[0,1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
        "axes[0,1].legend()\n",
        "\n",
        "# 3. An√°lisis de Overfitting\n",
        "colors = ['red' if x > 0.05 else 'green' for x in results_df['Overfitting']]\n",
        "axes[1,0].bar(results_df['Model'], results_df['Overfitting'], color=colors, alpha=0.7)\n",
        "axes[1,0].set_ylabel('Overfitting (Train - Val)')\n",
        "axes[1,0].set_title('An√°lisis de Overfitting', fontweight='bold')\n",
        "axes[1,0].tick_params(axis='x', rotation=45)\n",
        "axes[1,0].axhline(y=0.05, color='orange', linestyle='--', label='Threshold (0.05)')\n",
        "axes[1,0].legend()\n",
        "\n",
        "# 4. Desviaci√≥n Est√°ndar de CV\n",
        "axes[1,1].bar(results_df['Model'], results_df['CV_Std'], color='orange', alpha=0.7)\n",
        "axes[1,1].set_ylabel('CV Standard Deviation')\n",
        "axes[1,1].set_title('Estabilidad del Modelo (CV Std)', fontweight='bold')\n",
        "axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Rankings\n",
        "print(\"\\nüìà RANKINGS DE MODELOS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\nü•á Top 3 por CV Mean Accuracy:\")\n",
        "top_cv = results_df.nlargest(3, 'CV_Mean')[['Model', 'CV_Mean']]\n",
        "for i, (_, row) in enumerate(top_cv.iterrows(), 1):\n",
        "    print(f\"{i}. {row['Model']}: {row['CV_Mean']:.4f}\")\n",
        "\n",
        "print(\"\\nüéØ Top 3 por Validation Accuracy:\")\n",
        "top_val = results_df.nlargest(3, 'Validation_Accuracy')[['Model', 'Validation_Accuracy']]\n",
        "for i, (_, row) in enumerate(top_val.iterrows(), 1):\n",
        "    print(f\"{i}. {row['Model']}: {row['Validation_Accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Modelos con menor Overfitting:\")\n",
        "low_overfit = results_df.nsmallest(3, 'Overfitting')[['Model', 'Overfitting']]\n",
        "for i, (_, row) in enumerate(low_overfit.iterrows(), 1):\n",
        "    print(f\"{i}. {row['Model']}: {row['Overfitting']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Optimizaci√≥n de Hiperpar√°metros y Predicciones Finales\n",
        "\n",
        "Optimizaremos los hiperpar√°metros del mejor modelo y generaremos las predicciones finales para el conjunto de prueba.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizaci√≥n de hiperpar√°metros para los top 3 modelos\n",
        "print(\"üîß OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Configuraci√≥n de validaci√≥n cruzada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 1. Optimizar Random Forest (generalmente uno de los mejores)\n",
        "print(\"\\nüå≤ Optimizando Random Forest...\")\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    rf_params,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "rf_grid.fit(X_train, y_train)\n",
        "print(f\"‚úÖ Mejores par√°metros RF: {rf_grid.best_params_}\")\n",
        "print(f\"üéØ Mejor score RF: {rf_grid.best_score_:.4f}\")\n",
        "\n",
        "# 2. Optimizar XGBoost\n",
        "print(\"\\nüöÄ Optimizando XGBoost...\")\n",
        "xgb_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "xgb_grid = GridSearchCV(\n",
        "    xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "    xgb_params,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "print(f\"‚úÖ Mejores par√°metros XGB: {xgb_grid.best_params_}\")\n",
        "print(f\"üéØ Mejor score XGB: {xgb_grid.best_score_:.4f}\")\n",
        "\n",
        "# 3. Optimizar LightGBM\n",
        "print(\"\\nüí° Optimizando LightGBM...\")\n",
        "lgb_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'num_leaves': [15, 31, 63]\n",
        "}\n",
        "\n",
        "lgb_grid = GridSearchCV(\n",
        "    lgb.LGBMClassifier(random_state=42, verbosity=-1),\n",
        "    lgb_params,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "lgb_grid.fit(X_train, y_train)\n",
        "print(f\"‚úÖ Mejores par√°metros LGB: {lgb_grid.best_params_}\")\n",
        "print(f\"üéØ Mejor score LGB: {lgb_grid.best_score_:.4f}\")\n",
        "\n",
        "# Comparar modelos optimizados\n",
        "optimized_results = [\n",
        "    {'Model': 'Random Forest (Optimized)', 'CV_Score': rf_grid.best_score_},\n",
        "    {'Model': 'XGBoost (Optimized)', 'CV_Score': xgb_grid.best_score_},\n",
        "    {'Model': 'LightGBM (Optimized)', 'CV_Score': lgb_grid.best_score_}\n",
        "]\n",
        "\n",
        "optimized_df = pd.DataFrame(optimized_results)\n",
        "print(f\"\\nüìä RESULTADOS DE MODELOS OPTIMIZADOS\")\n",
        "print(\"=\"*50)\n",
        "display(optimized_df)\n",
        "\n",
        "# Seleccionar el mejor modelo optimizado\n",
        "best_optimized_idx = optimized_df['CV_Score'].idxmax()\n",
        "best_optimized_name = optimized_df.loc[best_optimized_idx, 'Model']\n",
        "best_optimized_score = optimized_df.loc[best_optimized_idx, 'CV_Score']\n",
        "\n",
        "if 'Random Forest' in best_optimized_name:\n",
        "    final_model = rf_grid.best_estimator_\n",
        "    model_scaler = None\n",
        "elif 'XGBoost' in best_optimized_name:\n",
        "    final_model = xgb_grid.best_estimator_\n",
        "    model_scaler = None\n",
        "else:  # LightGBM\n",
        "    final_model = lgb_grid.best_estimator_\n",
        "    model_scaler = None\n",
        "\n",
        "print(f\"\\nüèÜ MODELO FINAL SELECCIONADO: {best_optimized_name}\")\n",
        "print(f\"üéØ Score final: {best_optimized_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar el modelo final con todos los datos de entrenamiento\n",
        "print(\"\\nüéØ ENTRENANDO MODELO FINAL CON TODOS LOS DATOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Usar todo el conjunto de entrenamiento para el modelo final\n",
        "final_model.fit(X, y)\n",
        "\n",
        "# Generar predicciones para el conjunto de prueba\n",
        "if model_scaler is not None:\n",
        "    X_test_scaled = model_scaler.transform(X_test_final)\n",
        "    final_predictions = final_model.predict(X_test_scaled)\n",
        "else:\n",
        "    final_predictions = final_model.predict(X_test_final)\n",
        "\n",
        "print(f\"‚úÖ Predicciones generadas para {len(final_predictions)} muestras de prueba\")\n",
        "\n",
        "# Crear DataFrame de submission\n",
        "submission = pd.DataFrame({\n",
        "    'PassengerId': test_df['PassengerId'],\n",
        "    'Survived': final_predictions\n",
        "})\n",
        "\n",
        "# Guardar archivo de submission\n",
        "submission.to_csv('titanic_submission.csv', index=False)\n",
        "print(f\"üíæ Archivo de submission guardado: 'titanic_submission.csv'\")\n",
        "\n",
        "# Mostrar estad√≠sticas de las predicciones\n",
        "print(f\"\\nüìä ESTAD√çSTICAS DE PREDICCIONES\")\n",
        "print(\"=\"*40)\n",
        "survival_pred_counts = pd.Series(final_predictions).value_counts()\n",
        "survival_pred_percentage = pd.Series(final_predictions).value_counts(normalize=True) * 100\n",
        "\n",
        "print(f\"Predicciones de No Supervivencia (0): {survival_pred_counts.get(0, 0)} ({survival_pred_percentage.get(0, 0):.1f}%)\")\n",
        "print(f\"Predicciones de Supervivencia (1): {survival_pred_counts.get(1, 0)} ({survival_pred_percentage.get(1, 0):.1f}%)\")\n",
        "\n",
        "# Mostrar primeras 10 predicciones\n",
        "print(f\"\\nüìã PRIMERAS 10 PREDICCIONES\")\n",
        "print(\"-\"*30)\n",
        "display(submission.head(10))\n",
        "\n",
        "# An√°lisis de importancia de caracter√≠sticas (si el modelo lo soporta)\n",
        "if hasattr(final_model, 'feature_importances_'):\n",
        "    print(f\"\\nüîç IMPORTANCIA DE CARACTER√çSTICAS\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': final_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    print(\"Top 10 caracter√≠sticas m√°s importantes:\")\n",
        "    display(feature_importance.head(10))\n",
        "    \n",
        "    # Visualizar importancia de caracter√≠sticas\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(feature_importance.head(10)['Feature'][::-1], \n",
        "             feature_importance.head(10)['Importance'][::-1])\n",
        "    plt.xlabel('Importancia')\n",
        "    plt.title('Top 10 Caracter√≠sticas M√°s Importantes', fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(f\"\\nüéâ PROCESO COMPLETADO EXITOSAMENTE!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚úÖ Mejor modelo: {best_optimized_name}\")\n",
        "print(f\"üéØ Score de validaci√≥n cruzada: {best_optimized_score:.4f}\")\n",
        "print(f\"üìÅ Archivo de submission: 'titanic_submission.csv'\")\n",
        "print(f\"üöÄ ¬°Listo para subir a Kaggle!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Conclusiones y Resumen del Proyecto\n",
        "\n",
        "### üìä Resumen del Challenge T√©cnico\n",
        "\n",
        "Este proyecto ha implementado exitosamente un pipeline completo de machine learning para la competencia de Titanic en Kaggle, cumpliendo con todos los requisitos del challenge t√©cnico:\n",
        "\n",
        "#### ‚úÖ Objetivos Cumplidos\n",
        "\n",
        "1. **An√°lisis Exploratorio Completo (EDA)**\n",
        "   - Identificaci√≥n de patrones clave de supervivencia\n",
        "   - Visualizaciones comprehensivas\n",
        "   - An√°lisis de valores faltantes y outliers\n",
        "\n",
        "2. **Preprocesamiento Avanzado**\n",
        "   - Feature Engineering (t√≠tulos, tama√±o de familia, grupos de edad)\n",
        "   - Imputaci√≥n inteligente de valores faltantes\n",
        "   - Codificaci√≥n de variables categ√≥ricas\n",
        "   - Escalado apropiado para modelos que lo requieren\n",
        "\n",
        "3. **Implementaci√≥n de 6 Modelos**\n",
        "   - ‚úÖ Regresi√≥n Log√≠stica\n",
        "   - ‚úÖ K-Nearest Neighbors (KNN)\n",
        "   - ‚úÖ √Årbol de Decisi√≥n\n",
        "   - ‚úÖ Random Forest\n",
        "   - ‚úÖ XGBoost\n",
        "   - ‚úÖ LightGBM\n",
        "\n",
        "4. **Benchmark Comprehensivo**\n",
        "   - Validaci√≥n cruzada de 5 folds\n",
        "   - An√°lisis de overfitting\n",
        "   - Comparaci√≥n de m√©tricas m√∫ltiples\n",
        "   - Optimizaci√≥n de hiperpar√°metros\n",
        "\n",
        "5. **Generaci√≥n de Predicciones**\n",
        "   - Archivo de submission listo para Kaggle\n",
        "   - An√°lisis de importancia de caracter√≠sticas\n",
        "\n",
        "### üéØ Insights Principales\n",
        "\n",
        "1. **Factores m√°s importantes para la supervivencia:**\n",
        "   - G√©nero (las mujeres tuvieron mayor probabilidad de supervivencia)\n",
        "   - Clase socioecon√≥mica (primera clase > segunda > tercera)\n",
        "   - Edad (los ni√±os tuvieron mejor chance de supervivencia)\n",
        "   - Tarifa pagada (correlacionada con la clase)\n",
        "\n",
        "2. **Feature Engineering exitoso:**\n",
        "   - Extracci√≥n de t√≠tulos del nombre\n",
        "   - Creaci√≥n de grupos de tama√±o familiar\n",
        "   - Grupos de edad y tarifa\n",
        "   - Indicador de posesi√≥n de cabina\n",
        "\n",
        "3. **Rendimiento de modelos:**\n",
        "   - Los modelos ensemble (Random Forest, XGBoost, LightGBM) mostraron mejor rendimiento\n",
        "   - La optimizaci√≥n de hiperpar√°metros mejor√≥ significativamente los resultados\n",
        "   - Validaci√≥n cruzada demostr√≥ estabilidad en los resultados\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
